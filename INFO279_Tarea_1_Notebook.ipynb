{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d1f751",
   "metadata": {},
   "source": [
    "# INFO279 - Tarea 1: Tratamiento Automático del Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da86c65",
   "metadata": {},
   "source": [
    "\n",
    "## Desafíos:\n",
    "\n",
    "1. **Clasificación de noticias**: Construir un modelo capaz de clasificar noticias según su categoría temática.\n",
    "2. **Geolocalización de eventos**: Extraer información de noticias, incluyendo el evento principal, dirección y coordenadas geográficas (latitud, longitud).\n",
    "\n",
    "**Categorías**: sociedad, salud, politica, medioambiente, internacional, entretenimiento, economia, deportes, cultura, cienciatecnologia\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529f4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd4dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el dataset\n",
      "Dataset cargado\n",
      "Columnas combinadas\n",
      "Texto preprocesado\n",
      "Etiquetas codificadas\n",
      "Tokenizador BERT cargado\n",
      "Codificando los textos\n",
      "Longitud máxima de secuencias: 128\n",
      "Secuencias de entrada preparadas\n",
      "Capa de Embedding añadida\n",
      "Modelo definido\n",
      "Modelo compilado\n",
      "Epoch 1/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 103ms/step - accuracy: 0.2525 - loss: 1.9836 - val_accuracy: 0.0730 - val_loss: 8.6766\n",
      "Epoch 2/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 104ms/step - accuracy: 0.5549 - loss: 1.2947 - val_accuracy: 0.0707 - val_loss: 12.3208\n",
      "Epoch 3/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 104ms/step - accuracy: 0.6838 - loss: 0.9489 - val_accuracy: 0.0743 - val_loss: 13.8270\n",
      "Epoch 4/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 103ms/step - accuracy: 0.7559 - loss: 0.7290 - val_accuracy: 0.0614 - val_loss: 15.6756\n",
      "Epoch 5/5\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 104ms/step - accuracy: 0.8103 - loss: 0.5694 - val_accuracy: 0.0593 - val_loss: 18.1909\n",
      "Modelo entrenado durante 10 épocas\n",
      "Modelo guardado\n",
      "Modelo cargado\n",
      "Modelo compilado para evaluación\n",
      "\u001b[1m688/688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "Predicciones realizadas\n",
      "Accuracy: 0.7382272727272727\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     accidentes       0.83      0.97      0.89      2000\n",
      "        ciencia       0.71      0.90      0.80      2000\n",
      "       deportes       0.00      0.00      0.00      2000\n",
      "       economia       0.00      0.00      0.00      2000\n",
      "      educacion       0.81      0.87      0.84      2000\n",
      "entretenimiento       0.56      0.94      0.70      2000\n",
      "  internacional       0.93      0.90      0.92      2000\n",
      " medio_ambiente       0.62      0.96      0.75      2000\n",
      "       politica       0.87      0.96      0.91      2000\n",
      "          salud       0.72      0.76      0.74      2000\n",
      "     tecnologia       0.77      0.87      0.82      2000\n",
      "\n",
      "       accuracy                           0.74     22000\n",
      "      macro avg       0.62      0.74      0.67     22000\n",
      "   weighted avg       0.62      0.74      0.67     22000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\crist\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\crist\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Cargar modelo de SpaCy para procesamiento de texto en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Preprocesar texto: eliminar caracteres especiales y convertir a minúsculas\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Eliminar caracteres especiales\n",
    "    return text.lower()  # Convertir todo a minúsculas\n",
    "\n",
    "print(\"Cargando el dataset\")\n",
    "\n",
    "# Cargar dataset de ejemplo para clasificación\n",
    "df = pd.read_csv('data/train_data.csv')  # Cambiar por la ruta real del dataset\n",
    "print(\"Dataset cargado\")\n",
    "\n",
    "# Combinar las columnas 'title' y 'text' en una sola columna de contenido (si deseas)\n",
    "df['content'] = df['title'] + \" \" + df['text']\n",
    "print(\"Columnas combinadas\")\n",
    "\n",
    "# Preprocesar el dataset (título y texto)\n",
    "df['content'] = df['content'].apply(preprocess_text)\n",
    "print(\"Texto preprocesado\")\n",
    "\n",
    "# Codificación de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['clase'])\n",
    "print(\"Etiquetas codificadas\")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')  # Usar BERT\n",
    "print(\"Tokenizador BERT cargado\")\n",
    "\n",
    "\n",
    "def encode_text(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors=\"np\", max_length=128)\n",
    "\n",
    "print(\"Codificando los textos\")\n",
    "\n",
    "# Codificar los textos de entrenamiento\n",
    "encodings = encode_text(df['content'])\n",
    "\n",
    "# Obtener la longitud máxima de las secuencias para el padding\n",
    "max_length = 128  # Reducido a 128 para optimizar el tiempo de entrenamiento\n",
    "print(\"Longitud máxima de secuencias:\", max_length)\n",
    "\n",
    "# Padding de las secuencias\n",
    "X_pad = pad_sequences(encodings['input_ids'], maxlen=max_length, padding='post')\n",
    "y = df['label'].values\n",
    "print(\"Secuencias de entrada preparadas\")\n",
    "\n",
    "# Definir el modelo de red neuronal convolucional \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.vocab) + 1, output_dim=100))  # Usando embedding preentrenado si lo tienes\n",
    "print(\"Capa de Embedding añadida\")\n",
    "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))  # Regularización\n",
    "model.add(LSTM(units=32, return_sequences=True))  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  \n",
    "print(\"Modelo definido\")\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "print(\"Modelo compilado\")\n",
    "\n",
    "# Entrenar el modelo con todo el dataset durante 10 épocas completas\n",
    "model.fit(X_pad, y, epochs=5, batch_size=16, validation_split=0.2)  # Entrenará durante 10 épocas completas\n",
    "print(\"Modelo entrenado durante 10 épocas\")\n",
    "\n",
    "\n",
    "model.save(\"modelo_clasificador_noticias.keras\")\n",
    "print(\"Modelo guardado\")\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "loaded_model = load_model(\"modelo_clasificador_noticias.keras\")\n",
    "print(\"Modelo cargado\")\n",
    "\n",
    "# Compilar el modelo cargado (para evaluación)\n",
    "loaded_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Modelo compilado para evaluación\")\n",
    "\n",
    "# Predicción y evaluación en el mismo dataset\n",
    "y_pred = np.argmax(loaded_model.predict(X_pad), axis=-1)\n",
    "\n",
    "# Reporte de evaluación\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Predicciones realizadas\")\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(classification_report(y, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57f8e64-bdf6-4c7b-bf6b-1ff7bd37b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando spaCy...\n",
      "spaCy cargado.\n",
      "Geolocalizador inicializado.\n",
      "Cargando el modelo de clasificación...\n",
      "Modelo cargado.\n",
      "Cargando el dataset...\n",
      "Preprocesando el texto (combinando título y texto)...\n",
      "Texto combinado y preprocesado.\n",
      "Texto codificado...\n",
      "Obteniendo embeddings de BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma antes de la capa Dense: (100, 768)\n",
      "Forma de los embeddings ajustados: (100, 128)\n",
      "Prediciendo usando el modelo cargado...\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Extrayendo ubicaciones de los textos...\n",
      "Geolocalizando 'chile' para la noticia 10405...\n",
      "Geolocalizando 'coquimbo' para la noticia 20027...\n",
      "Geolocalizando '¿' para la noticia 2044...\n",
      "Geolocalizando 'temuco' para la noticia 11509...\n",
      "Geolocalizando 'chile' para la noticia 8399...\n",
      "Geolocalizando 'calle albany' para la noticia 15575...\n",
      "Geolocalizando 'rn ximena ossandón' para la noticia 11332...\n",
      "Geolocalizando 'talca' para la noticia 9093...\n",
      "Geolocalizando 'venezuela' para la noticia 14497...\n",
      "Geolocalizando 'lyon de providencia' para la noticia 4721...\n",
      "Geolocalizando 'coquimbo' para la noticia 10651...\n",
      "Geolocalizando 'betis' para la noticia 3474...\n",
      "Geolocalizando 'carolina' para la noticia 5355...\n",
      "Geolocalizando 'corea del sur' para la noticia 14627...\n",
      "Geolocalizando 'chile' para la noticia 7871...\n",
      "Geolocalizando 'chile' para la noticia 18842...\n",
      "Geolocalizando 'coquimbo' para la noticia 4294...\n",
      "Geolocalizando 'viena' para la noticia 16653...\n",
      "Geolocalizando 'capitán granate' para la noticia 12284...\n",
      "Geolocalizando 'museo de bomberos de santiago' para la noticia 16737...\n",
      "Geolocalizando 'chile' para la noticia 2146...\n",
      "Geolocalizando 'eeuu' para la noticia 3376...\n",
      "Geolocalizando 'venezuela' para la noticia 10983...\n",
      "Geolocalizando 'santiago' para la noticia 8116...\n",
      "Geolocalizando 'constitución' para la noticia 15323...\n",
      "Geolocalizando 'llegó' para la noticia 5381...\n",
      "Geolocalizando 'israel' para la noticia 2865...\n",
      "Geolocalizando 'venezuela' para la noticia 19686...\n",
      "Geolocalizando 'puente alto' para la noticia 4915...\n",
      "Geolocalizando 'buenos aires' para la noticia 11230...\n",
      "Geolocalizando 'noaa' para la noticia 15174...\n",
      "Geolocalizando 'chile' para la noticia 16215...\n",
      "Geolocalizando 'banco de chile' para la noticia 7593...\n",
      "Geolocalizando 'llau llao' para la noticia 3078...\n",
      "Geolocalizando 'inglaterra' para la noticia 14269...\n",
      "Geolocalizando 'garantía' para la noticia 5101...\n",
      "Geolocalizando 'cuál' para la noticia 7841...\n",
      "Geolocalizando 'florida' para la noticia 3753...\n",
      "Geolocalizando 'chile' para la noticia 17484...\n",
      "Geolocalizando 'lyon' para la noticia 7809...\n",
      "Geolocalizando 'chile' para la noticia 11910...\n",
      "Geolocalizando 'venus' para la noticia 7914...\n",
      "Geolocalizando 'venezuela' para la noticia 5199...\n",
      "Geolocalizando 'india' para la noticia 10688...\n",
      "Geolocalizando 'florida' para la noticia 4611...\n",
      "Geolocalizando 'matus' para la noticia 961...\n",
      "Geolocalizando 'garantía' para la noticia 2752...\n",
      "Geolocalizando 'bolivia' para la noticia 17596...\n",
      "Geolocalizando 'garantía' para la noticia 2782...\n",
      "Geolocalizando 'san josé' para la noticia 16335...\n",
      "Geolocalizando 'telemedicina' para la noticia 3911...\n",
      "Geolocalizando 'falla de san ramón' para la noticia 15581...\n",
      "Geolocalizando 'aysén' para la noticia 14817...\n",
      "Geolocalizando 'ministro' para la noticia 6246...\n",
      "Geolocalizando 'santiago' para la noticia 8733...\n",
      "Geolocalizando 'comuna de santiago' para la noticia 12963...\n",
      "Geolocalizando 'chile' para la noticia 3526...\n",
      "Geolocalizando 'garantía' para la noticia 35...\n",
      "Geolocalizando 'ordoñez' para la noticia 18243...\n",
      "Geolocalizando 'ministro pardow' para la noticia 13056...\n",
      "Geolocalizando 'través' para la noticia 11567...\n",
      "Geolocalizando 'houston' para la noticia 607...\n",
      "Geolocalizando 'mañana' para la noticia 1078...\n",
      "Geolocalizando 'recoleta' para la noticia 10520...\n",
      "Geolocalizando 'cerro navia' para la noticia 11864...\n",
      "Geolocalizando 'sifup' para la noticia 15845...\n",
      "Geolocalizando 'wisteria' para la noticia 9781...\n",
      "Geolocalizando 'coquimbo' para la noticia 3572...\n",
      "Geolocalizando 'junaeb' para la noticia 14662...\n",
      "Geolocalizando 'rosario' para la noticia 11447...\n",
      "Geolocalizando '¿' para la noticia 3512...\n",
      "Geolocalizando 'venezuela' para la noticia 14087...\n",
      "Geolocalizando 'chile' para la noticia 12401...\n",
      "Geolocalizando 'valdivia' para la noticia 4255...\n",
      "Geolocalizando 'chile' para la noticia 15682...\n",
      "Geolocalizando 'temuco' para la noticia 452...\n",
      "Geolocalizando 'temuco' para la noticia 804...\n",
      "Geolocalizando '‘' para la noticia 15279...\n",
      "Geolocalizando 'argentina' para la noticia 14250...\n",
      "Geolocalizando 'hospital san camilo' para la noticia 7589...\n",
      "Geolocalizando 'bazán' para la noticia 6137...\n",
      "Geolocalizando 'terapeuta' para la noticia 2502...\n",
      "Geolocalizando 'chadwick' para la noticia 4741...\n",
      "Geolocalizando 'minera barrick' para la noticia 16530...\n",
      "Geolocalización completada.\n",
      "Proceso completado y resultados guardados.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "print(\"Cargando spaCy...\")\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "print(\"spaCy cargado.\")\n",
    "\n",
    "# Inicializando geolocalizador\n",
    "print(\"Geolocalizador inicializado.\")\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "# Cargando el modelo de clasificación\n",
    "print(\"Cargando el modelo de clasificación...\")\n",
    "modelo_clasificador = tf.keras.models.load_model(\"modelo_clasificador_noticias.keras\")\n",
    "print(\"Modelo cargado.\")\n",
    "\n",
    "# Cargando el dataset\n",
    "print(\"Cargando el dataset...\")\n",
    "df = pd.read_csv('data/dataset_agosto2024.csv')\n",
    "\n",
    "# Seleccionar solo 100 noticias del dataset\n",
    "df = df.sample(n=100, random_state=42)\n",
    "\n",
    "# Preprocesamiento del texto (combinando título y texto)\n",
    "print(\"Preprocesando el texto (combinando título y texto)...\")\n",
    "df['texto_combinado'] = df['title'] + ' ' + df['text']\n",
    "df['texto_combinado'] = df['texto_combinado'].apply(lambda x: str(x).lower())\n",
    "print(\"Texto combinado y preprocesado.\")\n",
    "\n",
    "# Codificación de texto\n",
    "print(\"Texto codificado...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "inputs = tokenizer(df['texto_combinado'].tolist(), padding=True, truncation=True, return_tensors=\"tf\", max_length=512)\n",
    "\n",
    "\n",
    "print(\"Obteniendo embeddings de BERT...\")\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_embeddings = bert_model(inputs['input_ids'])[0]  \n",
    "\n",
    "\n",
    "cls_embeddings = bert_embeddings[:, 0, :]  \n",
    "\n",
    "# Ajustar la forma para que coincida con la entrada esperada por el modelo\n",
    "print(f\"Forma antes de la capa Dense: {cls_embeddings.shape}\")\n",
    "cls_embeddings_resized = tf.keras.layers.Dense(128, activation='relu')(cls_embeddings)\n",
    "\n",
    "# Verificando la forma de los embeddings ajustados\n",
    "print(f\"Forma de los embeddings ajustados: {cls_embeddings_resized.shape}\")\n",
    "\n",
    "# Predecir usando el modelo cargado\n",
    "print(\"Prediciendo usando el modelo cargado...\")\n",
    "predicciones = modelo_clasificador.predict(cls_embeddings_resized)\n",
    "\n",
    "# Decodificando las predicciones\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df['media_outlet'].unique()) \n",
    "\n",
    "predicciones_codificadas = np.argmax(predicciones, axis=1)\n",
    "predicciones_decodificadas = label_encoder.inverse_transform(predicciones_codificadas)\n",
    "\n",
    "# Añadir predicciones al DataFrame\n",
    "df['category'] = predicciones_decodificadas\n",
    "\n",
    "# Función para extraer eventos de las noticias\n",
    "def extraer_evento(texto):\n",
    "\n",
    "    return '.'.join(texto.split('.')[:1])\n",
    "\n",
    "df['event'] = df['texto_combinado'].apply(extraer_evento)\n",
    "\n",
    "# Función para extraer ubicaciones del texto usando spaCy\n",
    "def extraer_ubicaciones(texto):\n",
    "    doc = nlp(texto)\n",
    "    ubicaciones = [ent.text for ent in doc.ents if ent.label_ == 'LOC']  # Detectamos entidades etiquetadas como 'LOC' (lugares)\n",
    "    return ubicaciones[0] if ubicaciones else None\n",
    "\n",
    "# Aplicar la detección de ubicaciones al texto combinado\n",
    "print(\"Extrayendo ubicaciones de los textos...\")\n",
    "df['address'] = df['texto_combinado'].apply(extraer_ubicaciones)\n",
    "\n",
    "# Geolocalización basada en las ubicaciones detectadas\n",
    "df['latitud'] = np.nan\n",
    "df['longitud'] = np.nan\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    address = row['address']\n",
    "    if address:  # Si se detectó al menos una ubicación\n",
    "        print(f\"Geolocalizando '{address}' para la noticia {i+1}...\")\n",
    "        try:\n",
    "            location = geolocator.geocode(address)\n",
    "            if location:\n",
    "                df.at[i, 'latitud'] = location.latitude\n",
    "                df.at[i, 'longitud'] = location.longitude\n",
    "        except Exception as e:\n",
    "            print(f\"Error al geolocalizar '{address}': {e}\")\n",
    "\n",
    "print(\"Geolocalización completada.\")\n",
    "\n",
    "\n",
    "df_resultado = df[['id_news','event', 'category', 'address', 'latitud', 'longitud']]\n",
    "df_resultado.to_csv('resultados_geolocalizacion.csv', index=False)\n",
    "\n",
    "print(\"Proceso completado y resultados guardados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cbf0-fb12-412d-8b3a-cb638cdac861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722332f-40bc-4fab-b74f-b6a1c2e73898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
